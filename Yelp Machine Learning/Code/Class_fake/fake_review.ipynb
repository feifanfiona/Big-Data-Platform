{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Fake_detector').getOrCreate()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!hdfs dfs -mkdir /user/$USER/fake_review\n",
    "#!hdfs dfs -ls /user/$USER/\n",
    "#!hadoop dfs -put /home/$USER/final/test_no_label.csv     /user/$USER/fake_review\n",
    "#!hadoop dfs -put /home/$USER/final/train1.csv     /user/$USER/fake_review\n",
    "#!hadoop dfs -put /home/$USER/final/train2.csv     /user/$USER/fake_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data \n",
    "df_no_label = spark.read \\\n",
    "    .option(\"quote\", \"\\\"\")  \\\n",
    "    .option(\"escape\", \"\\\"\") \\\n",
    "    .option(\"ignoreLeadingWhiteSpace\",True) \\\n",
    "    .option(\"multiLine\", True) \\\n",
    "    .option(\"ignoreTrailingWhiteSpace\", True) \\\n",
    "    .csv(\"/user/ccy/fake_review/test_no_label.csv\",inferSchema=True, header=True)\n",
    "\n",
    "df_train1 = spark.read \\\n",
    "    .option(\"quote\", \"\\\"\")  \\\n",
    "    .option(\"escape\", \"\\\"\") \\\n",
    "    .option(\"ignoreLeadingWhiteSpace\",True) \\\n",
    "    .option(\"multiLine\", True) \\\n",
    "    .option(\"ignoreTrailingWhiteSpace\", True) \\\n",
    "    .csv(\"/user/ccy/fake_review/train1.csv\",inferSchema=True, header=True)\n",
    "\n",
    "df_train2 = spark.read \\\n",
    "    .option(\"quote\", \"\\\"\")  \\\n",
    "    .option(\"escape\", \"\\\"\") \\\n",
    "    .option(\"ignoreLeadingWhiteSpace\",True) \\\n",
    "    .option(\"multiLine\", True) \\\n",
    "    .option(\"ignoreTrailingWhiteSpace\", True) \\\n",
    "    .csv(\"/user/ccy/fake_review/train2.csv\",inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+------+-----+-------------------+--------------------+\n",
      "|ex_id|user_id|prod_id|rating|label|               date|              review|\n",
      "+-----+-------+-------+------+-----+-------------------+--------------------+\n",
      "|    6|    929|      0|   4.0| null|2009-08-25 00:00:00|Let me start with...|\n",
      "|    9|    932|      0|   5.0| null|2014-05-09 00:00:00|Stopped in for lu...|\n",
      "|   14|    937|      0|   4.0| null|2014-10-15 00:00:00|Tiny little place...|\n",
      "|   22|    945|      0|   5.0| null|2014-04-10 00:00:00|Food was deliciou...|\n",
      "|   23|    946|      0|   5.0| null|2014-03-29 00:00:00|Awesome hole in t...|\n",
      "|   24|    947|      0|   5.0| null|2014-03-21 00:00:00|I love this place...|\n",
      "|   25|    948|      0|   5.0| null|2014-03-08 00:00:00|The food is amazi...|\n",
      "|   31|    954|      0|   4.0| null|2013-10-02 00:00:00|Novelty meets Med...|\n",
      "|   33|    956|      0|   5.0| null|2013-08-22 00:00:00|If you're looking...|\n",
      "|   34|    957|      0|   4.0| null|2013-08-02 00:00:00|Great find in SoH...|\n",
      "+-----+-------+-------+------+-----+-------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_no_label.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+------+-----+-------------------+--------------------+\n",
      "|ex_id|user_id|prod_id|rating|label|               date|              review|\n",
      "+-----+-------+-------+------+-----+-------------------+--------------------+\n",
      "|    0|    923|      0|   3.0|    1|2014-12-08 00:00:00|The food at snack...|\n",
      "|    1|    924|      0|   3.0|    1|2013-05-16 00:00:00|This little place...|\n",
      "|    2|    925|      0|   4.0|    1|2013-07-01 00:00:00|ordered lunch for...|\n",
      "|    3|    926|      0|   4.0|    1|2011-07-28 00:00:00|This is a beautif...|\n",
      "|    4|    927|      0|   4.0|    1|2010-11-01 00:00:00|Snack is great pl...|\n",
      "|    5|    928|      0|   4.0|    1|2009-09-02 00:00:00|A solid 4 stars f...|\n",
      "|    7|    930|      0|   4.0|    1|2007-05-20 00:00:00|Love this place! ...|\n",
      "|    8|    931|      0|   4.0|    1|2005-12-27 00:00:00|My friend and I w...|\n",
      "|   10|    933|      0|   5.0|    1|2014-01-21 00:00:00|pretty cool place...|\n",
      "|   12|    935|      0|   5.0|    1|2011-01-31 00:00:00|Fabulous Authenti...|\n",
      "+-----+-------+-------+------+-----+-------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train1.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+------+-----+-------------------+--------------------+\n",
      "|ex_id|user_id|prod_id|rating|label|               date|              review|\n",
      "+-----+-------+-------+------+-----+-------------------+--------------------+\n",
      "|   11|    934|      0|   5.0|    1|2014-01-20 00:00:00|all around good p...|\n",
      "|   17|    940|      0|   4.0|    0|2014-09-16 00:00:00|For lunch, my fri...|\n",
      "|   20|    943|      0|   5.0|    0|2014-05-24 00:00:00|Some good Big Gre...|\n",
      "|   30|    953|      0|   4.0|    0|2013-10-17 00:00:00|So... as you may ...|\n",
      "|   43|    966|      0|   3.0|    0|2012-12-19 00:00:00|I don't understan...|\n",
      "|   53|    976|      0|   5.0|    0|2012-05-31 00:00:00|YUMS! I just trie...|\n",
      "|   56|    979|      0|   4.0|    0|2012-04-11 00:00:00|Delicious. ��Get ...|\n",
      "|   64|    987|      0|   4.0|    0|2011-12-18 00:00:00|We would love to ...|\n",
      "|   65|    988|      0|   3.0|    0|2011-12-13 00:00:00|Snack is an adora...|\n",
      "|   73|    996|      0|   4.0|    0|2011-09-18 00:00:00|Exploring SOHO an...|\n",
      "+-----+-------+-------+------+-----+-------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_label: 72165\n",
      "train1: 250874\n",
      "train2: 35918\n"
     ]
    }
   ],
   "source": [
    "print(\"no_label:\", df_no_label.count())\n",
    "print(\"train1:\", df_train1.count())\n",
    "print(\"train2:\", df_train2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|label| count|\n",
      "+-----+------+\n",
      "|    1| 25819|\n",
      "|    0|225055|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train1.select(\"label\").groupby(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train1.drop(*(\"ex_id\",\"user_id\",\"prod_id\",\"rating\",\"date\"))\n",
    "df_test  = df_train2.drop(*(\"ex_id\",\"user_id\",\"prod_id\",\"rating\",\"date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordopt(df):\n",
    "  #  text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    df = df.withColumn(\"review\", regexp_replace(col(\"review\"), '\\[.*?\\]', \" \"))     \n",
    "    df = df.withColumn(\"review\", regexp_replace(col(\"review\"), \"\\\\W\", \" \"))   \n",
    "    df = df.withColumn(\"review\", regexp_replace(col(\"review\"), 'https?://\\S+|www\\.\\S+', \" \"))   \n",
    "    df = df.withColumn(\"review\", regexp_replace(col(\"review\"), '<.*?>+', \" \"))       \n",
    "    df = df.withColumn(\"review\", regexp_replace(col(\"review\"), '\\n', \" \"))    \n",
    "    df = df.withColumn(\"review\", regexp_replace(col(\"review\"), '\\w*\\d\\w*', \" \"))\n",
    "    df = df.select(\"label\", lower(col(\"review\")).alias(\"review\"))\n",
    "    df = df.withColumn(\"review\", regexp_replace(col(\"review\"), \"[^A-Za-z]\", \" \"))\n",
    "    return df\n",
    "\n",
    "df_train = wordopt(df_train)\n",
    "df_test = wordopt(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train = df_train.withColumn(\"review\", regexp_replace(col(\"review\"), \"[^A-Za-z]\", \" \"))\n",
    "#df_test = df_test.withColumn(\"review\", regexp_replace(col(\"review\"), \"[^A-Za-z]\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|              review|\n",
      "+-----+--------------------+\n",
      "|    1|the food at snack...|\n",
      "|    1|this little place...|\n",
      "|    1|ordered lunch for...|\n",
      "+-----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"review\", outputCol=\"words\")  #tokenize words\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")  #remove stop words\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\")\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "word2Vec = Word2Vec(vectorSize=200, inputCol=\"filtered\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train \n",
    "pipeline = Pipeline(stages=[tokenizer, remover,hashingTF, idf])\n",
    "train = pipeline.fit(df_train).transform(df_train)\n",
    "train = train.drop(*(\"review\", \"words\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf])\n",
    "test = pipeline.fit(df_test).transform(df_test)\n",
    "test = test.drop(*(\"review\", \"words\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|label| count|\n",
      "+-----+------+\n",
      "|    1| 25819|\n",
      "|    0|225055|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.select(\"label\").groupby(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1| 3648|\n",
      "|    0|32270|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.select(\"label\").groupby(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_df = train.filter(col(\"label\") == 0)\n",
    "minor_df = train.filter(col(\"label\") == 1)\n",
    "ratio = int(major_df.count()/minor_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+\n",
      "|label|            filtered|         rawFeatures|            features|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "|    0|[braised, lamb, s...|(262144,[19889,22...|(262144,[19889,22...|\n",
      "|    0|[spot, close, job...|(262144,[2437,863...|(262144,[2437,863...|\n",
      "|    0|[needed, quick, ,...|(262144,[11996,16...|(262144,[11996,16...|\n",
      "|    0|[artichoke, chick...|(262144,[84488,11...|(262144,[84488,11...|\n",
      "|    0|[needed, quick, b...|(262144,[14,11996...|(262144,[14,11996...|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = range(ratio)\n",
    "# duplicate the minority rows\n",
    "oversampled_df = minor_df.withColumn(\"dummy\", explode(array([lit(x) for x in a]))).drop('dummy')\n",
    "# combine both oversampled minority rows and previous majority rows \n",
    "combined_df = major_df.unionAll(oversampled_df)\n",
    "combined_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.withColumn(\"label\",combined_df.label.cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|label| count|\n",
      "+-----+------+\n",
      "|  1.0|206552|\n",
      "|  0.0|225055|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_df.select(\"label\").groupby(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_majority_df = major_df.sample(False, 1/ratio)\n",
    "combined_df_2 = sampled_majority_df.unionAll(minor_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1|25819|\n",
      "|    0|28234|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_df_2.select(\"label\").groupby(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+\n",
      "|label|            filtered|         rawFeatures|            features|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "|    0|[delicious, lamb,...|(262144,[117396,1...|(262144,[117396,1...|\n",
      "|    0|[tiny, cafe, thom...|(262144,[5987,863...|(262144,[5987,863...|\n",
      "|    0|[impressed, , fir...|(262144,[5795,912...|(262144,[5795,912...|\n",
      "|    0|[lacklustre, serv...|(262144,[4200,139...|(262144,[4200,139...|\n",
      "|    0|[greek, , huge, f...|(262144,[19823,42...|(262144,[19823,42...|\n",
      "|    0|[followed, great,...|(262144,[5381,104...|(262144,[5381,104...|\n",
      "|    0|[trying, wait, he...|(262144,[31463,36...|(262144,[31463,36...|\n",
      "|    0|[stumbled, upon, ...|(262144,[6981,858...|(262144,[6981,858...|\n",
      "|    0|[friday, night, ,...|(262144,[14,991,8...|(262144,[14,991,8...|\n",
      "|    0|[small, simple, g...|(262144,[19823,34...|(262144,[19823,34...|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_df_2.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression Model\n",
    "lr1 = LogisticRegression() \n",
    "\n",
    "# Predict each point's label and show the results.\n",
    "lrm1 = lr1.fit(combined_df)\n",
    "predictions = lrm1.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.721615902889916\n",
      "0.7701399610539134\n"
     ]
    }
   ],
   "source": [
    "#print evaluation metrics\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"}))\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.631301392717162"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator_bi = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
    "auc = evaluator_bi.evaluate(predictions)\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0|26277|\n",
      "|       1.0| 9641|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"prediction\").groupby(\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Class Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "balancingRatio = train.filter(col(\"label\") == 0).count() / train.count()\n",
    "\n",
    "calculateWeights = udf(lambda x: 1 * balancingRatio if x == 0 else (1 * (1.0 - balancingRatio)))\n",
    "weightedDataset = train.withColumn(\"classWeightCol\", calculateWeights(\"label\"))\n",
    "weightedDataset = weightedDataset.withColumn(\"classWeightCol\",weightedDataset.classWeightCol.cast('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+-------------------+\n",
      "|label|            filtered|         rawFeatures|            features|     classWeightCol|\n",
      "+-----+--------------------+--------------------+--------------------+-------------------+\n",
      "|    1|[food, snack, sel...|(262144,[2437,978...|(262144,[2437,978...|0.10291620494750353|\n",
      "|    1|[little, place, s...|(262144,[19862,24...|(262144,[19862,24...|0.10291620494750353|\n",
      "+-----+--------------------+--------------------+--------------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weightedDataset.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression Model\n",
    "#lr = LogisticRegression().setWeightCol(\"classWeightCol\")\n",
    "\n",
    "lr = LogisticRegression(weightCol=\"classWeightCol\")\n",
    "\n",
    "# Predict each point's label and show the results.\n",
    "lrm = lr.fit(weightedDataset)\n",
    "predictions = lrm.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0|34482|\n",
      "|       1.0| 1436|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"prediction\").groupby(\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6375750588510144"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator_bi = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
    "auc = evaluator_bi.evaluate(predictions)\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8703157191380366\n",
      "0.8442522330060832\n"
     ]
    }
   ],
   "source": [
    "#print evaluation metrics\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"}))\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train model with Training Data\n",
    "rf = RandomForestClassifier()\n",
    "rfModel = rf.fit(combined_df_2)\n",
    "predictions = rfModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0|35891|\n",
      "|       1.0|   27|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"prediction\").groupby(\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6413191159841309"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator_bi = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
    "auc = evaluator_bi.evaluate(predictions)\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8979063422239546\n",
      "0.8503214121875211\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"}))\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lsvc = LinearSVC()\n",
    "rfcm = lsvc.fit(combined_df_2)\n",
    "predictions = rfcm.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"}))\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select(\"prediction\").groupby(\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+------+-----+-------------------+--------------------+\n",
      "|ex_id|user_id|prod_id|rating|label|               date|              review|\n",
      "+-----+-------+-------+------+-----+-------------------+--------------------+\n",
      "|    6|    929|      0|   4.0| null|2009-08-25 00:00:00|Let me start with...|\n",
      "|    9|    932|      0|   5.0| null|2014-05-09 00:00:00|Stopped in for lu...|\n",
      "|   14|    937|      0|   4.0| null|2014-10-15 00:00:00|Tiny little place...|\n",
      "|   22|    945|      0|   5.0| null|2014-04-10 00:00:00|Food was deliciou...|\n",
      "|   23|    946|      0|   5.0| null|2014-03-29 00:00:00|Awesome hole in t...|\n",
      "+-----+-------+-------+------+-----+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_no_label.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = df_no_label.select(\"review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordopt(df):\n",
    "  #  text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    df = df.withColumn(\"review\", regexp_replace(col(\"review\"), '\\[.*?\\]', \" \"))     \n",
    "    df = df.withColumn(\"review\", regexp_replace(col(\"review\"), \"\\\\W\", \" \"))   \n",
    "    df = df.withColumn(\"review\", regexp_replace(col(\"review\"), 'https?://\\S+|www\\.\\S+', \" \"))   \n",
    "    df = df.withColumn(\"review\", regexp_replace(col(\"review\"), '<.*?>+', \" \"))       \n",
    "    df = df.withColumn(\"review\", regexp_replace(col(\"review\"), '\\n', \" \"))    \n",
    "    df = df.withColumn(\"review\", regexp_replace(col(\"review\"), '\\w*\\d\\w*', \" \"))\n",
    "    df = df.select(lower(col(\"review\")).alias(\"review\"))\n",
    "    df = df.withColumn(\"review\", regexp_replace(col(\"review\"), \"[^A-Za-z]\", \" \"))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              review|\n",
      "+--------------------+\n",
      "|let me start with...|\n",
      "|stopped in for lu...|\n",
      "|tiny little place...|\n",
      "|food was deliciou...|\n",
      "|awesome hole in t...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table = wordopt(table)\n",
    "table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf])\n",
    "table2 = pipeline.fit(table).transform(table)\n",
    "table2 = table2.drop(*(\"review\", \"words\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|            filtered|         rawFeatures|            features|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|[let, start, shou...|(262144,[9886,125...|(262144,[9886,125...|\n",
      "|[stopped, lunch, ...|(262144,[24113,27...|(262144,[24113,27...|\n",
      "|[tiny, little, pl...|(262144,[54425,61...|(262144,[54425,61...|\n",
      "|[food, delicious,...|(262144,[22323,24...|(262144,[22323,24...|\n",
      "|[awesome, hole, w...|(262144,[14,7336,...|(262144,[14,7336,...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lrm.transform(table2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0|69235|\n",
      "|       1.0| 2930|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"prediction\").groupby(\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reveiws from Meet SPA and Tanta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = spark.createDataFrame(\n",
    "    [\n",
    "        (\"Meet SPA\", \"This place is brand new, and it is absolutely gorgeous inside. They are still building their website, so if you have any question, just call ahead.  I received a massage from Nina, it took me to a different planet. I fell asleep twice which is definitely a compliment. Pricing is very fair, and they have an array of services to choose from.\"),\n",
    "        (\"Meet SPA\",\"This spa is absolutely gorgeous and the owner and staff are incredibly kind and gracious. This was one of--if not the best--massage snd spa treatment I have ever experienced. I promise--you will not be disappointed!!!!\"),\n",
    "        (\"Meet SPA\",\"I found this place on Yelp. I was looking for a same day couples massage in the West Loop. I loved that the place was super clean and very easy to book. I tried the hot stone massage. They also play great relaxing music. I recommend calling to book an appointment. I couldn't figure out the website, however I received great customer service over the phone.\"),\n",
    "        (\"Tanta\",\"Came to Tanta for dinner with a friend  on the weekend . Very warm service and see up sitting at the bar where the bartenders were wonderful. Had some fancy cocktails , Ceviche and the Peruvian steak . Everything was tasty and the mood was festive ! Great choice for Latin inspired fusion with some sushi ( Japanese ) inspirations !\"),\n",
    "        (\"Tanta\", \"Try Lomo Saltado, Skirt Steak from the Nikkei bar and don't forget tres leches con lucuma. Maybe one if the best desserts I have tried! I didn't like the music that much but the food still deserves 5 stars\"),\n",
    "        (\"Tanta\",\"Loooove this place and definitely coming back again. Came here for a birthday dinner and the service was so nice. They even brought me out a little dessert and candle.The octopus was Devine. Perfectly grilled and cooked. The pork belly was amazing. The chicken skewers were good too but probably not the best dish in comparison to everything else we ordered.\")\n",
    "    ],\n",
    "    [\"id\", \"review\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|      id|              review|\n",
      "+--------+--------------------+\n",
      "|Meet SPA|This place is bra...|\n",
      "|Meet SPA|This spa is absol...|\n",
      "|Meet SPA|I found this plac...|\n",
      "|   Tanta|Came to Tanta for...|\n",
      "|   Tanta|Try Lomo Saltado,...|\n",
      "|   Tanta|Loooove this plac...|\n",
      "+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+\n",
      "|            filtered|         probability|prediction|\n",
      "+--------------------+--------------------+----------+\n",
      "|[place, brand, ne...|[0.58334630905852...|       0.0|\n",
      "|[spa, absolutely,...|[0.49604392569652...|       1.0|\n",
      "|[found, place, ye...|[0.44975949597738...|       1.0|\n",
      "|[came, tanta, din...|[0.79013395692662...|       0.0|\n",
      "|[try, lomo, salta...|[0.64433429476311...|       0.0|\n",
      "|[loooove, place, ...|[0.70299984302838...|       0.0|\n",
      "+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review = wordopt(review) \n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf])\n",
    "table2 = pipeline.fit(review).transform(review)\n",
    "table2 = table2.drop(*(\"review\", \"words\"))\n",
    "\n",
    "result = lrm1.transform(table2)\n",
    "result.select(\"filtered\",\"probability\",\"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
